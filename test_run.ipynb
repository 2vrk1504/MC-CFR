{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import games as games\n",
    "import game_utils as utils\n",
    "import game_utils_external_samp as external_sampling\n",
    "import game_tree as game_tree \n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "reload(games)\n",
    "reload(game_tree)\n",
    "reload(external_sampling)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Infosets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14482810/14482810 [01:36<00:00, 149540.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making TFDP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14482810/14482810 [00:16<00:00, 883411.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Infosets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8827459/8827459 [00:57<00:00, 154294.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making TFDP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8827459/8827459 [00:09<00:00, 911949.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# For test purposes\n",
    "# game_tfdp = game_tree.GameTFDP(game_class=games.RockPaperSuperScissors)\n",
    "# game_tfdp.build_tfdp_from_file(filename='rps_player0.txt', player=0)\n",
    "# game_tfdp.build_tfdp_from_file(filename='rps_player1.txt', player=1)\n",
    "\n",
    "game_tfdp = game_tree.GameTFDP(game_class=games.PhantomTicTacToe)\n",
    "game_tfdp.build_tfdp_from_file(filename='player0-infoset.txt', player=0)\n",
    "game_tfdp.build_tfdp_from_file(filename='player1-infoset.txt', player=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "def save_zip_file(numpy_tensor_filename, player: int):\n",
    "    zipfilename = f'pttt_pl{player}.zip'\n",
    "    with zipfile.ZipFile(zipfilename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(numpy_tensor_filename)\n",
    "    print(f\"Zip file '{zipfilename}' created successfully.\")\n",
    "    \n",
    "\n",
    "def save_policy(policy: game_tree.SparseSeqVec, player: int, game_tfdp: game_tree.GameTFDP, \n",
    "                infoset_filename: str, policy_name: str = \"\"):\n",
    "    lines = open(infoset_filename, \"r\").readlines()\n",
    "    array = np.zeros((len(lines), game_tfdp.game_class.NUM_ACTIONS))\n",
    "    for i, line in enumerate(lines):\n",
    "        infoset_label = line.strip()\n",
    "        infoset = game_tfdp.infoset_map[player][infoset_label]\n",
    "        dist = utils.normalize(policy[infoset])\n",
    "        for action in infoset.action_to_idx:\n",
    "            array[i][action] = dist[infoset.action_to_idx[action]]\n",
    "            \n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if len(policy_name) > 0:\n",
    "        filename = \"./policies/\" + policy_name + f\"{player}.npy\"\n",
    "    else:\n",
    "        filename = \"./policies/\" + f\"pttt_pl{player}_{timestamp}.npy\"\n",
    "    np.save(filename, array)\n",
    "    print(f\"Saved '{filename}' successfully.\")\n",
    "    save_zip_file(filename, player)\n",
    "\n",
    "def load_policy(numpy_tensor_filename: str, infoset_filename: str, game_tfdp: game_tree.GameTFDP, player: int):\n",
    "    policy = game_tree.SparseSeqVec(game_tfdp, player)\n",
    "    array = np.load(numpy_tensor_filename)\n",
    "    lines = open(infoset_filename, \"r\").readlines()\n",
    "    for i, line in enumerate(tqdm(lines)):\n",
    "        infoset_label = line.strip()\n",
    "        infoset = game_tfdp.infoset_map[player][infoset_label]\n",
    "        vec = np.zeros(len(infoset.idx_to_action))\n",
    "        for j in infoset.idx_to_action:\n",
    "            vec[j] = array[i][infoset.idx_to_action[j]]\n",
    "        policy[infoset] = vec\n",
    "    return policy\n",
    "\n",
    "def get_latest_policy_filenames(num_players: int):\n",
    "    policy_files = [\"\" for _ in range(num_players)]\n",
    "    for player in range(num_players):\n",
    "        policy_files[player] = sorted(glob(f\"./policies/pttt_pl{player}_*.npy\"))[-1]\n",
    "    return policy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn Nash Equilibria (Outcome Sampling) (call this)\n",
    "\n",
    "def outcome_sampling_learner(game_tfdp, load_policy_files: List[str] = []):\n",
    "    if load_policy_files:\n",
    "        average_policies = [None for _ in range(game_tfdp.num_players)]\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            average_policies[player] = load_policy(load_policy_files[player], f\"player{player}-infoset.txt\", game_tfdp, player)\n",
    "        current_policies = [average_policies[player].copy() for player in range(game_tfdp.num_players)]\n",
    "    else:\n",
    "        current_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "        average_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "\n",
    "    cum_regret = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "    cum_reaches = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "    print(\"Expectation\")\n",
    "    ev_list = [utils.compute_expected_value(game_tfdp, 0, average_policies)]\n",
    "    print(\"Nash Gap\")\n",
    "    gap_list = [utils.nash_conv(game_tfdp, average_policies)[0]]\n",
    "    T = 10000 # change this to your liking\n",
    "    pbar = tqdm(range(T))\n",
    "    for episode in pbar:\n",
    "        tfdp_subtrees = [{}, {}]\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            cf_value, tfdp_subtrees[player] = utils.compute_counterfactual_value(game_tfdp, player, current_policies)\n",
    "            cf_regret = cf_value - (cf_value * current_policies[player]).reduce(np.sum).sparseseqvec()\n",
    "            current_policies[player] = utils.CFR(game_tfdp, player, current_policies, cf_regret, cum_regret[player], tfdp_subtrees[player])\n",
    "\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            cum_reaches[player] = utils.get_reach_probability(game_tfdp, player, current_policies[player], tfdp_subtrees[player]) / (episode + 1.) \\\n",
    "                + cum_reaches[player] * episode / (episode + 1.)\n",
    "            average_policies[player] = cum_reaches[player].map(utils.normalize)\n",
    "\n",
    "        iir = 0.1\n",
    "        ev = iir * utils.compute_expected_value(game_tfdp, 0, average_policies) + ev_list[-1] * (1-iir)\n",
    "        nash_gap = iir * utils.nash_conv(game_tfdp, average_policies)[0] + gap_list[-1] * (1-iir) # exponential averaging\n",
    "        ev_list.append(ev)\n",
    "        gap_list.append(nash_gap)\n",
    "        \n",
    "        pbar.set_description(f\"Expected utility {ev:.5f}, Nash gap: {nash_gap:.5f}\")\n",
    "    pbar.close()\n",
    "\n",
    "    for player in range(game_tfdp.num_players):\n",
    "        save_policy(average_policies[player], player, game_tfdp, f\"player{player}-infoset.txt\")\n",
    "        \n",
    "    \n",
    "    return average_policies, ev_list, gap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_policies, ev_list, gap_list = outcome_sampling_learner(game_tfdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy for player 0 from ./policies\\pttt_pl0_20241128_045719.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14482810/14482810 [01:39<00:00, 145733.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Loading policy for player 1 from ./policies\\pttt_pl1_20241128_045919.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8827459/8827459 [01:03<00:00, 139199.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Initial Num nodes: [14482810, 8827459], After pruning: [1570499, 4099189]\n"
     ]
    }
   ],
   "source": [
    "# tree pruning (Works :D)\n",
    "load_policy_files = get_latest_policy_filenames(game_tfdp.num_players)\n",
    "average_policies = [None for _ in range(game_tfdp.num_players)]\n",
    "for player in range(game_tfdp.num_players):\n",
    "    print(f\"Loading policy for player {player} from {load_policy_files[player]}\")\n",
    "    average_policies[player] = load_policy(load_policy_files[player], f\"player{player}-infoset.txt\", game_tfdp, player)\n",
    "    print(\"Done.\")\n",
    "\n",
    "utils.prune_tfdp(game_tfdp, average_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn Nash Equilibria with external sampling (call This)\n",
    "# better to run after pruning the game_TFDP\n",
    "# this method is expected to give better performance\n",
    "\n",
    "def external_sampling_learner(game_tfdp: game_tree.GameTFDP, load_policy_files: List[str] = []):\n",
    "    if load_policy_files:\n",
    "        average_policies = [None for _ in range(game_tfdp.num_players)]\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            average_policies[player] = load_policy(load_policy_files[player], f\"player{player}-infoset.txt\", game_tfdp, player)\n",
    "        current_policies = [average_policies[player].copy() for player in range(game_tfdp.num_players)]\n",
    "    else:\n",
    "        current_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "        average_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "\n",
    "    cum_regret = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "    cum_reaches = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "    print(\"Expectation\")\n",
    "    ev_list = [external_sampling.compute_expected_value(game_tfdp, 0, average_policies)]\n",
    "    print(\"Nash Gap\")\n",
    "    gap_list = [external_sampling.nash_conv(game_tfdp, average_policies)[0]]\n",
    "    T = 50 # change this to your liking, start with small numbers\n",
    "    pbar = tqdm(range(T))\n",
    "    for episode in pbar:\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            cf_value = external_sampling.compute_counterfactual_value(game_tfdp, player, current_policies)\n",
    "            cf_regret = cf_value - (cf_value * current_policies[player]).reduce(np.sum).sparseseqvec()\n",
    "            current_policies[player] = external_sampling.CFR(game_tfdp, player, current_policies, cf_regret, cum_regret[player])\n",
    "\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            cum_reaches[player] = external_sampling.get_reach_probability(game_tfdp, player, current_policies[player]) / (episode + 1.) \\\n",
    "                + cum_reaches[player] * episode / (episode + 1.)\n",
    "            average_policies[player] = cum_reaches[player].map(utils.normalize)\n",
    "\n",
    "        iir = 0.1\n",
    "        ev = iir * external_sampling.compute_expected_value(game_tfdp, 0, average_policies) + ev_list[-1] * (1-iir)\n",
    "        nash_gap = iir * external_sampling.nash_conv(game_tfdp, average_policies)[0] + gap_list[-1] * (1-iir) # exponential averaging\n",
    "        ev_list.append(ev)\n",
    "        gap_list.append(nash_gap)\n",
    "        \n",
    "        pbar.set_description(f\"Expected utility {ev:.5f}, Nash gap: {nash_gap:.5f}\")\n",
    "    pbar.close()  \n",
    "    \n",
    "    for player in range(game_tfdp.num_players):\n",
    "        save_policy(average_policies[player], player, game_tfdp, f\"player{player}-infoset.txt\")\n",
    "\n",
    "    return average_policies, ev_list, gap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_policies, ev_list, gap_list = external_sampling_learner(game_tfdp, get_latest_policy_filenames(game_tfdp.num_players))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('utility')\n",
    "ax.plot(ev_list)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('Nash gap')\n",
    "ax.plot(gap_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - Learning against Uniform policy\n",
    "print(\"Loading Current Policy\")\n",
    "current_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "cum_regret = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "cum_reaches = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "print(\"Loading Average Policy\")\n",
    "average_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize)  for player in range(game_tfdp.num_players)]\n",
    "print(\"Computing Expected value\")\n",
    "ev_list = [utils.compute_expected_value(game_tfdp, 0, average_policies)]\n",
    "print(ev_list)\n",
    "\n",
    "T = 200\n",
    "pbar = tqdm(range(T))\n",
    "for episode in pbar:\n",
    "    player = 0\n",
    "    cf_value, tfdp_subtree = utils.compute_counterfactual_value(game_tfdp, player, current_policies)\n",
    "    cf_regret = cf_value - (cf_value * current_policies[player]).reduce(np.sum).sparseseqvec()\n",
    "    current_policies[player] = utils.CFR(game_tfdp, player, current_policies, cf_regret, cum_regret[player], tfdp_subtree)\n",
    "\n",
    "    cum_reaches[player] = utils.get_reach_probability(game_tfdp, player, current_policies[player], tfdp_subtree) / (episode + 1.) \\\n",
    "        + cum_reaches[player] * episode / (episode + 1.)\n",
    "    average_policies[player] = cum_reaches[player].map(utils.normalize)\n",
    "\n",
    "    ev = utils.compute_expected_value(game_tfdp, 0, average_policies)\n",
    "    ev_list.append(ev)\n",
    "\n",
    "    pbar.set_description(f\"Expected utility {ev:.5f}\")\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
