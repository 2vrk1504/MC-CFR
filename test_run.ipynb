{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import games as games\n",
    "import game_utils as utils\n",
    "import game_utils_external_samp as external_sampling\n",
    "import game_tree as game_tree \n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "reload(games)\n",
    "reload(game_tree)\n",
    "reload(external_sampling)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Infosets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14482810/14482810 [01:37<00:00, 148146.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making TFDP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14482810/14482810 [00:17<00:00, 827500.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Infosets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8827459/8827459 [00:58<00:00, 150888.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making TFDP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8827459/8827459 [00:09<00:00, 889321.18it/s] \n"
     ]
    }
   ],
   "source": [
    "# For test purposes\n",
    "# game_tfdp = game_tree.GameTFDP(game_class=games.RockPaperSuperScissors)\n",
    "# game_tfdp.build_tfdp_from_file(filename='rps_player0.txt', player=0)\n",
    "# game_tfdp.build_tfdp_from_file(filename='rps_player1.txt', player=1)\n",
    "\n",
    "game_tfdp = game_tree.GameTFDP(game_class=games.PhantomTicTacToe)\n",
    "game_tfdp.build_tfdp_from_file(filename='player0-infoset.txt', player=0)\n",
    "game_tfdp.build_tfdp_from_file(filename='player1-infoset.txt', player=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "def save_zip_file(numpy_tensor_filename, player: int):\n",
    "    zipfilename = f'pttt_pl{player}.zip'\n",
    "    with zipfile.ZipFile(zipfilename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(numpy_tensor_filename)\n",
    "    print(f\"Zip file '{zipfilename}' created successfully.\")\n",
    "    \n",
    "\n",
    "def save_policy(policy: game_tree.SparseSeqVec, player: int, game_tfdp: game_tree.GameTFDP, \n",
    "                infoset_filename: str, policy_name: str = \"\"):\n",
    "    lines = open(infoset_filename, \"r\").readlines()\n",
    "    array = np.zeros((len(lines), game_tfdp.game_class.NUM_ACTIONS))\n",
    "    for i, line in enumerate(lines):\n",
    "        infoset_label = line.strip()\n",
    "        infoset = game_tfdp.infoset_map[player][infoset_label]\n",
    "        dist = utils.normalize(policy[infoset])\n",
    "        for action in infoset.action_to_idx:\n",
    "            array[i][action] = dist[infoset.action_to_idx[action]]\n",
    "            \n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if len(policy_name) > 0:\n",
    "        filename = \"./policies/\" + policy_name + f\"{player}.npy\"\n",
    "    else:\n",
    "        filename = \"./policies/\" + f\"pttt_pl{player}_{timestamp}.npy\"\n",
    "    np.save(filename, array)\n",
    "    print(f\"Saved '{filename}' successfully.\")\n",
    "    save_zip_file(filename, player)\n",
    "\n",
    "def load_policy(numpy_tensor_filename: str, infoset_filename: str, game_tfdp: game_tree.GameTFDP, player: int):\n",
    "    policy = game_tree.SparseSeqVec(game_tfdp, player)\n",
    "    array = np.load(numpy_tensor_filename)\n",
    "    lines = open(infoset_filename, \"r\").readlines()\n",
    "    for i, line in enumerate(tqdm(lines)):\n",
    "        infoset_label = line.strip()\n",
    "        infoset = game_tfdp.infoset_map[player][infoset_label]\n",
    "        vec = array[i][array[i] > 0]\n",
    "        if len(vec) > 0:\n",
    "            policy[infoset] = vec\n",
    "    return policy\n",
    "\n",
    "def get_latest_policy_filenames(num_players: int):\n",
    "    policy_files = [\"\" for _ in range(num_players)]\n",
    "    for player in range(num_players):\n",
    "        policy_files[player] = sorted(glob(f\"./policies/pttt_pl{player}_*.npy\"))[-1]\n",
    "    return policy_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn Nash Equilibria (Outcome Sampling) (call this)\n",
    "\n",
    "def outcome_sampling_learner(game_tfdp, load_policy_files: List[str] = []):\n",
    "    if load_policy_files:\n",
    "        average_policies = [None for _ in range(game_tfdp.num_players)]\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            average_policies[player] = load_policy(load_policy_files[player], f\"player{player}-infoset.txt\", game_tfdp, player)\n",
    "        current_policies = [average_policies[player].copy() for player in range(game_tfdp.num_players)]\n",
    "    else:\n",
    "        current_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "        average_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "\n",
    "    cum_regret = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "    cum_reaches = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "    print(\"Expectation\")\n",
    "    ev_list = [utils.compute_expected_value(game_tfdp, 0, average_policies)]\n",
    "    print(\"Nash Gap\")\n",
    "    gap_list = [utils.nash_conv(game_tfdp, average_policies)[0]]\n",
    "    T = 10000 # change this to your liking\n",
    "    pbar = tqdm(range(T))\n",
    "    for episode in pbar:\n",
    "        tfdp_subtrees = [{}, {}]\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            cf_value, tfdp_subtrees[player] = utils.compute_counterfactual_value(game_tfdp, player, current_policies)\n",
    "            cf_regret = cf_value - (cf_value * current_policies[player]).reduce(np.sum).sparseseqvec()\n",
    "            current_policies[player] = utils.CFR(game_tfdp, player, current_policies, cf_regret, cum_regret[player], tfdp_subtrees[player])\n",
    "\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            cum_reaches[player] = utils.get_reach_probability(game_tfdp, player, current_policies[player], tfdp_subtrees[player]) / (episode + 1.) \\\n",
    "                + cum_reaches[player] * episode / (episode + 1.)\n",
    "            average_policies[player] = cum_reaches[player].map(utils.normalize)\n",
    "\n",
    "        iir = 0.1\n",
    "        ev = iir * utils.compute_expected_value(game_tfdp, 0, average_policies) + ev_list[-1] * (1-iir)\n",
    "        nash_gap = iir * utils.nash_conv(game_tfdp, average_policies)[0] + gap_list[-1] * (1-iir) # exponential averaging\n",
    "        ev_list.append(ev)\n",
    "        gap_list.append(nash_gap)\n",
    "        \n",
    "        pbar.set_description(f\"Expected utility {ev:.5f}, Nash gap: {nash_gap:.5f}\")\n",
    "    pbar.close()\n",
    "\n",
    "    for player in range(game_tfdp.num_players):\n",
    "        save_policy(average_policies[player], player, game_tfdp, f\"player{player}-infoset.txt\")\n",
    "        \n",
    "    \n",
    "    return average_policies, ev_list, gap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading policy for player 0 from ./policies\\pttt_pl0_20241128_045719.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14482810/14482810 [01:40<00:00, 144000.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Loading policy for player 1 from ./policies\\pttt_pl1_20241128_045919.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8827459/8827459 [01:01<00:00, 143514.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "player local: 0, Player Global: 0\n",
      "Infoset: P1->|0*, Infoset Actions: {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7}\n",
      "idx_to_action: {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8}\n",
      "player local: 0, Player Global: 0\n",
      "Infoset: P1->|, Infoset Actions: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8}\n",
      "idx_to_action: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8}\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Windows\\Desktop\\Vallabh\\6.S890\\game_utils.py\u001b[0m in \u001b[0;36mtraverse\u001b[1;34m(seq, player_local)\u001b[0m\n\u001b[0;32m    221\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfoset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx_to_action\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m                                         \u001b[1;32mif\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m                                                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfoset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx_to_action\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 6",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Windows\\Desktop\\Vallabh\\6.S890\\game_utils.py\u001b[0m in \u001b[0;36mtraverse\u001b[1;34m(seq, player_local)\u001b[0m\n\u001b[0;32m    233\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfoset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx_to_action\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                                         \u001b[0mtraverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfoset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_infosets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer_local\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Windows\\Desktop\\Vallabh\\6.S890\\game_utils.py\u001b[0m in \u001b[0;36mtraverse\u001b[1;34m(seq, player_local)\u001b[0m\n\u001b[0;32m    238\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"idx_to_action: {infoset.idx_to_action}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b0d46f194bd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprune_tfdp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame_tfdp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_policies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Windows\\Desktop\\Vallabh\\6.S890\\game_utils.py\u001b[0m in \u001b[0;36mprune_tfdp\u001b[1;34m(game_tfdp, policies)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame_tfdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_players\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[0minitial_num_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_tfdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_infosets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m                 \u001b[0mtraverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgame_tfdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Initial Num nodes: {initial_num_nodes}, After pruning: {pruned_num_nodes}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Windows\\Desktop\\Vallabh\\6.S890\\game_utils.py\u001b[0m in \u001b[0;36mtraverse\u001b[1;34m(seq, player_local)\u001b[0m\n\u001b[0;32m    237\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Infoset: {infoset}, Infoset Actions: {infoset.action_to_idx}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"idx_to_action: {infoset.idx_to_action}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame_tfdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_players\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tree pruning\n",
    "load_policy_files = get_latest_policy_filenames(game_tfdp.num_players)\n",
    "average_policies = [None for _ in range(game_tfdp.num_players)]\n",
    "for player in range(game_tfdp.num_players):\n",
    "    print(f\"Loading policy for player {player} from {load_policy_files[player]}\")\n",
    "    average_policies[player] = load_policy(load_policy_files[player], f\"player{player}-infoset.txt\", game_tfdp, player)\n",
    "    print(\"Done.\")\n",
    "\n",
    "utils.prune_tfdp(game_tfdp, average_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn Nash Equilibria with external sampling (call This)\n",
    "# better to run after pruning the game_TFDP\n",
    "# this method is expected to give better perfromance\n",
    "\n",
    "def external_sampling_learner(game_tfdp: game_tree.GameTFDP, load_policy_files: List[str] = []):\n",
    "    if load_policy_files:\n",
    "        average_policies = [None for _ in range(game_tfdp.num_players)]\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            average_policies[player] = load_policy(load_policy_files[player], f\"player{player}-infoset.txt\", game_tfdp, player)\n",
    "        current_policies = [average_policies[player].copy() for player in range(game_tfdp.num_players)]\n",
    "    else:\n",
    "        current_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "        average_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "\n",
    "    cum_regret = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "    cum_reaches = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "    print(\"Expectation\")\n",
    "    ev_list = [external_sampling.compute_expected_value(game_tfdp, 0, average_policies)]\n",
    "    print(\"Nash Gap\")\n",
    "    gap_list = [external_sampling.nash_conv(game_tfdp, average_policies)[0]]\n",
    "    T = 50 # change this to your liking, start with small numbers\n",
    "    pbar = tqdm(range(T))\n",
    "    for episode in pbar:\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            cf_value = external_sampling.compute_counterfactual_value(game_tfdp, player, current_policies)\n",
    "            cf_regret = cf_value - (cf_value * current_policies[player]).reduce(np.sum).sparseseqvec()\n",
    "            current_policies[player] = external_sampling.CFR(game_tfdp, player, current_policies, cf_regret, cum_regret[player])\n",
    "\n",
    "        for player in range(game_tfdp.num_players):\n",
    "            cum_reaches[player] = external_sampling.get_reach_probability(game_tfdp, player, current_policies[player]) / (episode + 1.) \\\n",
    "                + cum_reaches[player] * episode / (episode + 1.)\n",
    "            average_policies[player] = cum_reaches[player].map(utils.normalize)\n",
    "\n",
    "        iir = 0.1\n",
    "        ev = iir * external_sampling.compute_expected_value(game_tfdp, 0, average_policies) + ev_list[-1] * (1-iir)\n",
    "        nash_gap = iir * external_sampling.nash_conv(game_tfdp, average_policies)[0] + gap_list[-1] * (1-iir) # exponential averaging\n",
    "        ev_list.append(ev)\n",
    "        gap_list.append(nash_gap)\n",
    "        \n",
    "        pbar.set_description(f\"Expected utility {ev:.5f}, Nash gap: {nash_gap:.5f}\")\n",
    "        \n",
    "        for player in range(game_tfdp.num_players):\n",
    "            save_policy(average_policies[player], player, game_tfdp, f\"player{player}-infoset.txt\")\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('utility')\n",
    "ax.plot(ev_list)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('iteration')\n",
    "ax.set_ylabel('Nash gap')\n",
    "ax.plot(gap_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test - Learning against Uniform policy\n",
    "print(\"Loading Current Policy\")\n",
    "current_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize) for player in range(game_tfdp.num_players)]\n",
    "cum_regret = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "cum_reaches = [game_tree.SparseSeqVec(game_tfdp, player) for player in range(game_tfdp.num_players)]\n",
    "print(\"Loading Average Policy\")\n",
    "average_policies = [game_tree.SparseSeqVec(game_tfdp, player).map(utils.normalize)  for player in range(game_tfdp.num_players)]\n",
    "print(\"Computing Expected value\")\n",
    "ev_list = [utils.compute_expected_value(game_tfdp, 0, average_policies)]\n",
    "print(ev_list)\n",
    "\n",
    "T = 200\n",
    "pbar = tqdm(range(T))\n",
    "for episode in pbar:\n",
    "    player = 0\n",
    "    cf_value, tfdp_subtree = utils.compute_counterfactual_value(game_tfdp, player, current_policies)\n",
    "    cf_regret = cf_value - (cf_value * current_policies[player]).reduce(np.sum).sparseseqvec()\n",
    "    current_policies[player] = utils.CFR(game_tfdp, player, current_policies, cf_regret, cum_regret[player], tfdp_subtree)\n",
    "\n",
    "    cum_reaches[player] = utils.get_reach_probability(game_tfdp, player, current_policies[player], tfdp_subtree) / (episode + 1.) \\\n",
    "        + cum_reaches[player] * episode / (episode + 1.)\n",
    "    average_policies[player] = cum_reaches[player].map(utils.normalize)\n",
    "\n",
    "    ev = utils.compute_expected_value(game_tfdp, 0, average_policies)\n",
    "    ev_list.append(ev)\n",
    "\n",
    "    pbar.set_description(f\"Expected utility {ev:.5f}\")\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
